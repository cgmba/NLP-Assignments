{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-iPZFoQiasf"
      },
      "source": [
        "## Lab 2: Text Classification\n",
        "\n",
        "Note: For this lab exercise, it is recommended that you use [Google colab](https://colab.research.google.com/) to avoid issues concerning the deep learning module dependencies on your local system.\n",
        "\n",
        "For questions contact:\n",
        "\n",
        "Yash Pawar\n",
        "\n",
        "email ID: yash.pawar@dsv.su.se"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugaJVbFBraxp"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "\n",
        "In this lab exercise, we will perform classification of text into predefined classes using Machine Learning. In particular, we will be classifying the text from [BBC](http://mlg.ucd.ie/datasets/bbc.html) dataset consisting of 5 different classes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GU2tn5d_raxu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "## Suppress warnings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE14c8aIraxv"
      },
      "source": [
        "### 2. Import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZjVIjeHXraxw",
        "outputId": "37bd9bc6-1a57-4460-89df-d9bf311b97ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-06075d71-8f0c-4a51-ba0f-773198512b4d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06075d71-8f0c-4a51-ba0f-773198512b4d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-06075d71-8f0c-4a51-ba0f-773198512b4d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-06075d71-8f0c-4a51-ba0f-773198512b4d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        category                                               text\n",
              "0           tech  tv future in the hands of viewers with home th...\n",
              "1       business  worldcom boss  left books alone  former worldc...\n",
              "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3          sport  yeading face newcastle in fa cup premiership s...\n",
              "4  entertainment  ocean s twelve raids box office ocean s twelve..."
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Note: The filepath has been specified considerning that the notebook is run using google colab.\n",
        "\n",
        "bbc = pd.read_csv(filepath_or_buffer='/content/bbc_text.csv', delimiter = ',')\n",
        "#bbc = pd.read_csv(\"bbc_text.csv\", delimiter = ',')\n",
        "bbc.head()\n",
        "#bbc.set_axis(['sno','category','text'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "082tyASTraxw",
        "outputId": "1455e4fa-6027-4a02-d82d-cd30929892f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of            category                                               text\n",
              "0              tech  tv future in the hands of viewers with home th...\n",
              "1          business  worldcom boss  left books alone  former worldc...\n",
              "2             sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3             sport  yeading face newcastle in fa cup premiership s...\n",
              "4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
              "...             ...                                                ...\n",
              "2220       business  cars pull down us retail figures us retail sal...\n",
              "2221       politics  kilroy unveils immigration policy ex-chatshow ...\n",
              "2222  entertainment  rem announce new glasgow concert us band rem h...\n",
              "2223       politics  how political squabbles snowball it s become c...\n",
              "2224          sport  souness delight at euro progress boss graeme s...\n",
              "\n",
              "[2225 rows x 2 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ## To do: Replace the ??? with code to split the dataset into train and test set\n",
        "# training_bbc, test_bbc, y_training_bbc, y_test_bbc = train_test_split(bbc.text, bbc.category, test_size=0.33, random_state=0)\n",
        "# #training_bbc, test_bbc = nlp.load_dataset('bbc_text', split=['train', 'test'])\n",
        "bbc.category.value_counts()\n",
        "#bbc['category_num'] = bbc.category.map({'sport':0, 'business':1, 'politics':2, 'tech':3, 'entertainment':4})\n",
        "bbc.head\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9S8k1zAdPvZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4007a0da-51f0-44eb-8d73-c7feb000d93a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(           category                                               text\n",
              " 793            tech  doors open at biggest gadget fair thousands of...\n",
              " 155   entertainment  the comic book genius of stan lee stan lee  th...\n",
              " 942           sport  butler strikes gold in spain britain s kathy b...\n",
              " 1293  entertainment  spector facing more legal action music produce...\n",
              " 139        business  rich grab half colombia poor fund half of the ...\n",
              " ...             ...                                                ...\n",
              " 2014          sport  desailly backs blues revenge trip marcel desai...\n",
              " 2157          sport  woodward eyes brennan for lions toulouse s for...\n",
              " 1931          sport  venus stunned by farina elia venus williams su...\n",
              " 1504  entertainment  global release for japan hit film oscar-winnin...\n",
              " 1712       politics  mps demand  budget leak  answers ministers hav...\n",
              " \n",
              " [1490 rows x 2 columns],\n",
              "            category                                               text\n",
              " 836        business  tsunami cost hits jakarta shares the stock mar...\n",
              " 925        business  man utd to open books to glazer manchester uni...\n",
              " 1425          sport  iaaf launches fight against drugs the iaaf - a...\n",
              " 29    entertainment  halloween writer debra hill dies screenwriter ...\n",
              " 298   entertainment  bafta to hand out movie honours movie stars fr...\n",
              " ...             ...                                                ...\n",
              " 176   entertainment  carry on star patsy rowlands dies actress pats...\n",
              " 215           sport  navratilova hits out at critics martina navrat...\n",
              " 1767          sport  downing injury mars uefa victory middlesbrough...\n",
              " 179        business  arsenal  may seek full share listing  arsenal ...\n",
              " 824        politics  prime minister s questions so who  if anyone  ...\n",
              " \n",
              " [735 rows x 2 columns])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "X = bbc.text\n",
        "#y = bbc.category_num\n",
        "y = bbc.category\n",
        "# print(X.shape)\n",
        "# print(y.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## To do: Replace the ??? with code to split the dataset into train and test set\n",
        "#training_bbc, test_bbc, y_training_bbc, y_test_bbc = train_test_split(X,y, test_size=0.2, random_state=43)\n",
        "training_bbc, test_bbc = train_test_split(bbc, test_size=0.33, random_state=50)\n",
        "# print(training_bbc.head(5))\n",
        "# print(test_bbc.head(5))\n",
        "# print(y_training_bbc.head(5))\n",
        "# print(y_test_bbc.head(5))\n",
        "\n",
        "training_bbc, test_bbc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUhRukvlraxx"
      },
      "source": [
        "### 3. Visualization\n",
        "\n",
        "Your task here is to get an understanding of distribution of different classes in the data by visualization and compare them.\n",
        "\n",
        "You are expected to generate two plots, on for each training and test dataset.\n",
        "\n",
        "You can refer to the [Bar plots tutorial](https://pythonguides.com/matplotlib-plot-bar-chart/) to know more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "S15Sb8iWraxx",
        "outputId": "8c91fd29-2ec1-4dbd-9a40-99ed2642e52d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# To do: add the code below to plot the Distribution of classes in both the datasets.\n",
        "# fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# ax[0].boxplot(test_bbc)\n",
        "# ax[0].set_title(\"TrainingBBC\")\n",
        "\n",
        "# ax[1].set_title('test_bbc')\n",
        "# ax[1].boxplot(training_bbc)\n",
        "\n",
        "bbc.groupby('category').count().plot.bar(ylim=0)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJeBNjcKrax1"
      },
      "source": [
        "### 5. Classification using Naive Bayes\n",
        "\n",
        "For training and validation, we will use a [Multinomial Naive Bayes classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html). Here, you are expected to:\n",
        "\n",
        "1. Vectorize the text from the training set.\n",
        "2. Train the classifier\n",
        "3. Evaluate the classifier using the test set. \n",
        "\n",
        "Tip: You can use [sklearn's pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) functionality to perform steps 1 and 2. \n",
        "\n",
        "Tip: You can use [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to print the results of evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PEXADlvrax2",
        "outputId": "ff0f2569-fb3c-4e1d-9823-95cbd61f6f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.98      0.96      0.97       182\n",
            "entertainment       0.99      0.95      0.97       143\n",
            "     politics       0.94      0.98      0.96       133\n",
            "        sport       0.99      0.99      0.99       157\n",
            "         tech       0.94      0.98      0.96       120\n",
            "\n",
            "     accuracy                           0.97       735\n",
            "    macro avg       0.97      0.97      0.97       735\n",
            " weighted avg       0.97      0.97      0.97       735\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# Train and evaluate a Multinomial Naive Bayes classifier\n",
        "# To do: Add the code below to build a pipeline for the classifier.\n",
        "\n",
        "\n",
        "#cv = CountVectorizer()\n",
        "#cv_model = cv.fit_transform(training_bbc)\n",
        "#training_bbc_x = cv.fit_transform(training_bbc)\n",
        "#test_bbc_x = cv.transform(test_bbc)\n",
        "\n",
        "\n",
        "\n",
        "#mnb = MultinomialNB()\n",
        "#%time mnb.fit(cv_model, training_bbc)\n",
        "#%time mnb.fit(cv_model, training_bbc[:1])\n",
        "#y_predition = mnb.predict(cv_model)\n",
        "\n",
        "text_clf = Pipeline([\n",
        " ('cv', CountVectorizer(stop_words='english')), ('predictor', MultinomialNB())\n",
        "])\n",
        "\n",
        "text_clf.fit(training_bbc['text'],training_bbc['category'])\n",
        "#text_clf.fit(X,y)\n",
        "y_predition = text_clf.predict(test_bbc)\n",
        "y_predition = text_clf.predict(test_bbc['text'])\n",
        "# svc = SVC\n",
        "# text_clf = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
        "\n",
        "\n",
        "#print(classification_report(y_test_bbc[:], y_predition))\n",
        "\n",
        "print(classification_report(test_bbc['category'], y_predition))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQR91FiQrax2"
      },
      "source": [
        "### 6. Baseline Classifier\n",
        "\n",
        "You can compare the performance of your Machine Learning model with a simple baseline classifier. One possibility could be to use a classifier that generates predictions by respecting the training set’s class distribution. You can consider using [Dummy classifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) from scikit learn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZunaVeFrax3",
        "outputId": "cd7065f1-8a9d-4cbd-c921-337b4fb0fb99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.24      0.19      0.21       182\n",
            "entertainment       0.17      0.15      0.16       143\n",
            "     politics       0.21      0.22      0.21       133\n",
            "        sport       0.24      0.27      0.26       157\n",
            "         tech       0.14      0.17      0.15       120\n",
            "\n",
            "     accuracy                           0.20       735\n",
            "    macro avg       0.20      0.20      0.20       735\n",
            " weighted avg       0.20      0.20      0.20       735\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "# Evaluate the random baseline\n",
        "baseline = DummyClassifier(strategy=\"stratified\")\n",
        "\n",
        "# To do: Add the code below to train the baseline classifier and evaluate it.\n",
        "\n",
        "#use f1 score in case of uneven distribution or unbalanced dataset\n",
        "baseline.fit(training_bbc['text'], training_bbc['category'])\n",
        "#baseline.fit(X,y)\n",
        "by_predition = baseline.predict(test_bbc[:])\n",
        "by_predition = baseline.predict(test_bbc['text'])\n",
        "#baseline.score(X, y)\n",
        "\n",
        "#print(classification_report(y_test_bbc, y_predition))\n",
        "print(classification_report(test_bbc['category'], by_predition))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hoa-P1QIrax4"
      },
      "source": [
        "Is the result from the baseline classifier justified?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwzvJ6qMrax4"
      },
      "source": [
        "### 6. Grid Search\n",
        "\n",
        "So far, you have trained the vectorizer and the classifer using their default parameters. However, in practical settings, one needs to optimize the parameters of the model to maximize the performance. \n",
        "\n",
        "Here, you are asked to find the optimal parameters for the pipelines that you have created above using a 5 fold cross validation. The choice of hyperparameters for optimization are:\n",
        "\n",
        "1. Bi-grams vs uni-grams vs tri-grams from [Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). \n",
        "2. Additive smoothing  for the Multinomial naive bayes classifier $\\alpha$ = {1, 0.1}\n",
        "3. Tokenized vs non-tokenized text (For tokenization, you can use the function 'preprocess' that is given below as a parameter for the vectorizer.)\n",
        "\n",
        "\n",
        "You can refer to the [Grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) documentation from the scikit-learn library.\n",
        "\n",
        "Finally, print the parameters from the grid search that give the best performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "fdhGS2HMU3Ac"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function preprocess can be used as a tokenizer.\n",
        "#from spacy.lang.en import English\n",
        "import spacy\n",
        "#import en_core_web_sm\n",
        "#from spacy.lang.en import English\n",
        "#nlp = en_core_web_sm.load()\n",
        "nlp = spacy.load('en_core_web_sm', disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "#nlp = spacy.load('en', disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        " \n",
        "    final_key=[]\n",
        "    for token in doc:\n",
        "        if token.is_stop==False and token.lemma_.isalpha():\n",
        "            \n",
        "            final_key.append(token.lemma_)\n",
        "        \n",
        "    return final_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOz9OcIlrax4",
        "outputId": "4ada837b-4ce9-40f4-baf4-3ead94b41c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------\n",
            "0.9778523489932885\n",
            "{'cv__ngram_range': (1, 3), 'cv__tokenizer': <function preprocess at 0x7f723b8289e0>, 'predictor__alpha': 0.1}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# To do: Replace the ??? in the code and implement the grid search\n",
        "# Note: Take a look at how you an specify the parameters for grid search from an example of n-grams. Similarly, you can specify the other remaining parameters.\n",
        "params = {'cv__ngram_range':[(1,1), (1,2), (1,3)],\n",
        "           'cv__tokenizer' :[None, preprocess],\n",
        "    'predictor__alpha': [1, 0.1]\n",
        "          }\n",
        "\n",
        "grid_search = GridSearchCV(text_clf, params)\n",
        "grid_search.fit(training_bbc['text'], training_bbc['category'])\n",
        "#grid_search.fit(X,y)\n",
        "print(\"-----------\")\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_params_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8ODDYpS626Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d36162-3564-4a1d-93ef-0794e8e6f187"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['cv', 'error_score', 'estimator__memory', 'estimator__steps', 'estimator__verbose', 'estimator__scaler', 'estimator__svc', 'estimator__scaler__copy', 'estimator__scaler__with_mean', 'estimator__scaler__with_std', 'estimator__svc__C', 'estimator__svc__break_ties', 'estimator__svc__cache_size', 'estimator__svc__class_weight', 'estimator__svc__coef0', 'estimator__svc__decision_function_shape', 'estimator__svc__degree', 'estimator__svc__gamma', 'estimator__svc__kernel', 'estimator__svc__max_iter', 'estimator__svc__probability', 'estimator__svc__random_state', 'estimator__svc__shrinking', 'estimator__svc__tol', 'estimator__svc__verbose', 'estimator', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "grid_search.get_params().keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GISqsrKD8md0"
      },
      "source": [
        "## 7. Fine-tuning using BERT\n",
        "\n",
        "In this section, you will see how a pre-trained BERT model can be fine tuned for the task of text classification. \n",
        "\n",
        "Run the following cells to fine-tune the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-zjPdPiZLEG",
        "outputId": "91c1515a-1c30-4550-89ee-6c42b36a7c7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[?25l\r\u001b[K     |████████                        | 10 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 41 kB 82 kB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.62.3)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30535 sha256=ac1dc645e9917d6fcf97a968898cc1b818affdf3a80394ad2fc6d9ecb3bf2186\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/b6/e5/8c76ec779f54bc5c2f1b57d2200bb9c77616da83873e8acb53\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19472 sha256=79561860a2de29b41ad5204f429772dfdac3ebd4b0661dcd5d4ecb8fe5dd7ad0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7911 sha256=e4b1c2384c693cae26111d90e15a32faefc0a399b7382aef51cf285fd4050858\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/11/67/33cc51bbee127cb8fb2ba549cd29109b2f22da43ddf9969716\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "'Download the tokenization script'\n",
        "# #!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "# #!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "# #!wget https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "# !wget  https://github.com/google-research/bert/blob/master/tokenization.py\n",
        "# !pip install sentencepiece\n",
        "\n",
        "#!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "\n",
        "\n",
        "\n",
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wks6VNo68xk-",
        "outputId": "98228cd2-2b27-4cee-d655-fa173cbc297b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.8.0\n",
            "Hub version:  0.12.0\n"
          ]
        }
      ],
      "source": [
        "'Import all the necessary modules'\n",
        "# !pip install bert-tensorflow\n",
        "# #!pip install bert-tensorflow==1.0.1\n",
        "# !pip install bert-for-tf2\n",
        "# #!pip install sentencepiece\n",
        "# #!pip install bert-tensorflow\n",
        "# !pip install tensorflow==2.0\n",
        "# from bert import tokenization\n",
        "\n",
        "# # from tensorflow.keras.layers import Dense, Input\n",
        "# # from tensorflow.keras.optimizers import Adam\n",
        "# # from tensorflow.keras.models import Model\n",
        "# # from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "# #import tokenization\n",
        "# import tensorflow as tf\n",
        "# import tensorflow_hub as hub\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "# from sklearn import preprocessing\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "'Import all the necessary modules'\n",
        "\n",
        "#import tokenization\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "from tensorflow.keras.models import  Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"TensorFlow Version:\",tf.__version__)\n",
        "print(\"Hub version: \",hub.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "eCiSoIU28656"
      },
      "outputs": [],
      "source": [
        "# 'Download the pretrained BERT model'\n",
        "# # !pip install tensorflow-io\n",
        "# # import tensorflow_io as tfio\n",
        "# m_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
        "# bert_layer = hub.KerasLayer(m_url, trainable=True)\n",
        "# #schema = tf.io.gfile.GFile('train.avsc').read()\n",
        "\n",
        "'Download the pretrained BERT model'\n",
        "\n",
        "m_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
        "bert_layer = hub.KerasLayer(m_url, trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "zK-Skk-K87oz"
      },
      "outputs": [],
      "source": [
        "#!pip install bert_tokenization\n",
        "# import bert_tokenizer as tokenizer\n",
        "# #from bert import tokenization\n",
        "# #import tokenization\n",
        "# # from pytorch_pretrained_bert import BertTokenizer\n",
        "# # tokenizer = BertTokenizer.from_pretrained('./dataset/bert-base-english')\n",
        "\n",
        "# vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "# do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "# tokenizer1 = tokenizer.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "# def bert_encode(texts, tokenizer, max_len=512):\n",
        "#     all_tokens = []\n",
        "#     all_masks = []\n",
        "#     all_segments = []\n",
        "    \n",
        "#     for text in texts:\n",
        "#         text = tokenizer1.tokenize(text)\n",
        "        \n",
        "#         text = text[:max_len-2]\n",
        "#         input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "#         pad_len = max_len-len(input_sequence)\n",
        "        \n",
        "#         tokens = tokenizer1.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
        "#         pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "#         segment_ids = [0] * max_len\n",
        "        \n",
        "#         all_tokens.append(tokens)\n",
        "#         all_masks.append(pad_masks)\n",
        "#         all_segments.append(segment_ids)\n",
        "        \n",
        "#     return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
        "\n",
        "# #import bert\n",
        "# from bert import tokenization\n",
        "# #from bert import bert_tokenization\n",
        "# #!pip install bert-tokenizer\n",
        "# #import bert_tokenizer as tokenizer\n",
        "# import tokenization\n",
        "# vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "# do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "# tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "# #tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "# def bert_encode(texts, tokenizer, max_len=512):\n",
        "#     all_tokens = []\n",
        "#     all_masks = []\n",
        "#     all_segments = []\n",
        "    \n",
        "#     for text in texts:\n",
        "#         text = tokenizer.tokenize(text)\n",
        "        \n",
        "#         text = text[:max_len-2]\n",
        "#         input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "#         pad_len = max_len-len(input_sequence)\n",
        "        \n",
        "#         tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
        "#         pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "#         segment_ids = [0] * max_len\n",
        "        \n",
        "#         all_tokens.append(tokens)\n",
        "#         all_masks.append(pad_masks)\n",
        "#         all_segments.append(segment_ids)\n",
        "        \n",
        "#     return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
        "\n",
        "\n",
        "\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "#tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "'Use BERT tokenizer'\n",
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "tokenizer=FullTokenizer(vocab_file,do_lower_case)\n",
        "\n",
        "\n",
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "        \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len-len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "        \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "hF_gDNZn8-LP"
      },
      "outputs": [],
      "source": [
        "# def build_model(bert_layer, max_len=512):\n",
        "#     input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "#     input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "#     segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    \n",
        "#     pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    \n",
        "#     clf_output = sequence_output[:, 0, :]\n",
        "    \n",
        "#     lay = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
        "#     lay = tf.keras.layers.Dropout(0.2)(lay)\n",
        "#     lay = tf.keras.layers.Dense(32, activation='relu')(lay)\n",
        "#     lay = tf.keras.layers.Dropout(0.2)(lay)\n",
        "#     out = tf.keras.layers.Dense(5, activation='softmax')(lay)\n",
        "    \n",
        "#     model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "#     model.compile(tf.keras.optimizers.Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "#     return model\n",
        "\n",
        "\n",
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    \n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    \n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    \n",
        "    lay = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
        "    lay = tf.keras.layers.Dropout(0.2)(lay)\n",
        "    lay = tf.keras.layers.Dense(32, activation='relu')(lay)\n",
        "    lay = tf.keras.layers.Dropout(0.2)(lay)\n",
        "    out = tf.keras.layers.Dense(5, activation='softmax')(lay)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "eM05qXjS9Bbj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "'Set the maximum length of the sequence'\n",
        "max_len = 512\n",
        "\n",
        "'Transform non-numerical labels to numerical'\n",
        "label = preprocessing.LabelEncoder()\n",
        "train_labels = label.fit_transform(training_bbc['category'])\n",
        "train_labels = to_categorical(train_labels)\n",
        "\n",
        "'Prepare the input by tokenising and padding the text sequence'\n",
        "train_input = bert_encode(training_bbc.text.values, tokenizer, max_len=max_len)\n",
        "test_input = bert_encode(test_bbc.text.values, tokenizer, max_len=max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0yJERe39F1F",
        "outputId": "c7bcbd96-f10c-4c0f-9e3e-8c07cf519ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['business' 'entertainment' 'politics' 'sport' 'tech']\n"
          ]
        }
      ],
      "source": [
        "labels = label.classes_\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HudNWXyS9L5z",
        "outputId": "f9106406-960f-4047-b1d5-a7457758e7a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_word_ids (InputLayer)    [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_mask (InputLayer)        [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " segment_ids (InputLayer)       [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " keras_layer_1 (KerasLayer)     [(None, 768),        109482241   ['input_word_ids[0][0]',         \n",
            "                                 (None, 512, 768)]                'input_mask[0][0]',             \n",
            "                                                                  'segment_ids[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['keras_layer_1[1][1]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 64)           49216       ['tf.__operators__.getitem_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 64)           0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 32)           2080        ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 32)           0           ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 5)            165         ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,533,702\n",
            "Trainable params: 109,533,701\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "'Build the model'\n",
        "\n",
        "model = build_model(bert_layer, max_len=max_len)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxd085OY9Rn4",
        "outputId": "611db1f9-812d-4cf2-c973-000bef5ccfd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "336/336 [==============================] - ETA: 0s - loss: 0.5043 - accuracy: 0.8210\n",
            "Epoch 1: val_accuracy improved from -inf to 0.98658, saving model to model.h5\n",
            "336/336 [==============================] - 363s 1s/step - loss: 0.5043 - accuracy: 0.8210 - val_loss: 0.0790 - val_accuracy: 0.9866\n",
            "Epoch 2/5\n",
            "336/336 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9642\n",
            "Epoch 2: val_accuracy did not improve from 0.98658\n",
            "336/336 [==============================] - 347s 1s/step - loss: 0.1355 - accuracy: 0.9642 - val_loss: 0.0948 - val_accuracy: 0.9597\n",
            "Epoch 3/5\n",
            "336/336 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 0.9732\n",
            "Epoch 3: val_accuracy did not improve from 0.98658\n",
            "336/336 [==============================] - 354s 1s/step - loss: 0.1041 - accuracy: 0.9732 - val_loss: 0.0324 - val_accuracy: 0.9866\n",
            "Epoch 4/5\n",
            "336/336 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9911\n",
            "Epoch 4: val_accuracy did not improve from 0.98658\n",
            "336/336 [==============================] - 347s 1s/step - loss: 0.0490 - accuracy: 0.9911 - val_loss: 0.1450 - val_accuracy: 0.9597\n",
            "Epoch 5/5\n",
            "336/336 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9955\n",
            "Epoch 5: val_accuracy improved from 0.98658 to 0.99329, saving model to model.h5\n",
            "336/336 [==============================] - 352s 1s/step - loss: 0.0290 - accuracy: 0.9955 - val_loss: 0.0387 - val_accuracy: 0.9933\n"
          ]
        }
      ],
      "source": [
        "'Start training the model'\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "\n",
        "train_sh = model.fit(\n",
        "    train_input, train_labels,\n",
        "    validation_split=0.1,\n",
        "    epochs=5,\n",
        "    callbacks=[checkpoint, earlystopping],\n",
        "    batch_size=4,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "I2Zr2lw1ClM7"
      },
      "outputs": [],
      "source": [
        "'Predict the classes from the fine-tuned BERT model'\n",
        "bert_pred = model.predict(test_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "_E9ufij-DU2q"
      },
      "outputs": [],
      "source": [
        "'Invert the classes from numerical to non-numerical (original) categories'\n",
        "y_pred_bert = label.inverse_transform(np.argmax(bert_pred.round().astype(int), axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnni25tiC-Ng",
        "outputId": "eaffe9b4-4f6f-4c36-8ea2-b828ea13d3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business     0.9942    0.9451    0.9690       182\n",
            "entertainment     1.0000    1.0000    1.0000       143\n",
            "     politics     0.9778    0.9925    0.9851       133\n",
            "        sport     0.9752    1.0000    0.9874       157\n",
            "         tech     0.9675    0.9917    0.9794       120\n",
            "\n",
            "     accuracy                         0.9837       735\n",
            "    macro avg     0.9829    0.9858    0.9842       735\n",
            " weighted avg     0.9839    0.9837    0.9836       735\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#print(classification_report(training_bbc,  y_pred_bert, digits=4))\n",
        "\n",
        "print(classification_report(test_bbc['category'],  y_pred_bert, digits=4))\n",
        "#len(y_pred_bert)\n",
        "# from pprint import pprint\n",
        "#'''training_bbc['category']'''\n",
        "#printable = classification_report(test_bbc.values, y_pred_bert, digits=4)\n",
        "#pprint(printable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyNKl7Z6_Sj5"
      },
      "source": [
        "1. Comment on the results. Is there any improvement in performance when compared to MultinomialNB?\n",
        "\n",
        "2. Try changing the number of epochs to 3 and then 5 to see if there is any improvement in the performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLev7gYhF_D2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Text_classification_final (1).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}